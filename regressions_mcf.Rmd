---
author:
- Matthias Farngruber
- Matriculation Number 1
- Filip Lukijanovic
- 11776896
- Peter Prlleshi
- Matriculation Number 3
subtitle: A sentiment analysis on the S&P 500 Stock Market Index
output:
  pdf_document:
    latex_engine: pdflatex
  html_document: default
title: "Does FED Communication cause immediate and abnormal returns stock market?"
nocite: '@*'
bibliography: references.bib
---

```{=tex}
\tableofcontents
\pagebreak
```
## Abstract {#sec-abstract}

\pagebreak

## Literature Review {#sec-literature-review}

The following research project is mostly based on the work done by Möller and Reichmann (2021) in the field of sentiment analysis. In their paper "ECB Language and Stock Returns - A Textual analysis of ECB Press Conferences" they explore the impact of the language used by the ECB in their regular press conferences on stock returns in the Euro Area.

They achieve this by mining the statements and ranking the sentiments of each statement based on general tone, uncertainty and constraint. Once this is classified, the researchers then cross-check the high frequency intraday data for Euro Area stock returns on each statement day, by employing the technique of an event study. This allows them to see how stock returns reacted to statements by the ECB at whatever point they wanted to examine them.

As mentioned, the researchers focused on tone, uncertainty and constraining language for the sentiment analysis. Tone captures the overall language - or the overarching narrative - of a statement, uncertainty measures how ambiguous a statement may be and constraining language quantifies how constraining the ECB communicates to be in the future. Of course the researchers did not read through every single statement by the ECB, instead, they employed a dictionary-based sentiment mining approach that considered grammatical and syntactical cues to analyze the sentiment expressed in ECB press conferences. Afterwards they scored each statement with regards to each category by employing heuristic-adjusted sentiment scores based on word lists used in previous studies. @möller2021

## Data {#sec-data}

The data used for this project consists of the following:

-   Federal Reserve (FED) Meetings: The U.S. American FED holds regular meetings multiple times per year where they talk about the current economic situation and what the plan is going forwards. This leaves us with 76 statements over 10 years from January 30th 2013 until July 26th 2023.

-   Standard & Poor's 500 stock market index pricing: We cross-check the statements and their nature with the price of the S&P 500 stock market index on each day of a statement. This index is of particular interest since it incorporates 500 U.S. american companies representing a large part of the whole market itself. This means that we can use the S&P 500 as a proxy for how the market behaves at a given day.

## Methodology {#sec-methodology}

```{r library setup, include=FALSE}
library(readxl)
library(tidyr)
library(zoo)
library(dplyr)
library(stargazer)
library(lmtest)
```

```{r Definition of Data, include=FALSE}

setwd(dirname(rstudioapi::getSourceEditorContext()$path))
path<-dirname(rstudioapi::getSourceEditorContext()$path)

Core <- read_excel("./SENTIMENTS_FOMC_FINAL_with_BERT.xlsx")
Core$Date <- as.Date(Core$Date, format = "%d/%m/%Y") # converting the date column
FilterDates<-c(Core$Date)
Core <- Core[, -2]

#################################################################################
Fed_Futures <- read_excel("./FED_FORWARDS.xlsx")
Fed_Futures$Date <- as.Date(Fed_Futures$Date, format="%Y-%m-%d %H:%M:%S")

Fed_Futures<- Fed_Futures %>% 
  mutate(Date = as.Date(Date)) %>%  
  complete(Date = seq.Date(min(Date), max(Date), by="day"))
      
Fed_Futures<-na.locf(Fed_Futures, fromLast = FALSE)
Fed_Futures<- Fed_Futures[Fed_Futures$Date %in% FilterDates,]

#####################################################################################

IR <- read.csv("./DFF.csv")
IR$Date <- as.Date(IR$DATE, format = "%Y-%m-%d") # converting the date column

IR<- IR %>% 
  mutate(Date = as.Date(Date)) %>%  
  complete(Date = seq.Date(min(Date), max(Date), by="day"))

IR <-na.locf(IR, fromLast = FALSE)
IR <- IR[IR$Date %in% FilterDates,][,-1]

#########################################################################

SPX_DE <- read_excel("./SPX_DE.xlsx")

SPX_DE$Date <- as.Date(SPX_DE$Date, format="%Y-%m-%d %H:%M:%S")

SPX_DE<- SPX_DE %>% 
  mutate(Date = as.Date(Date)) %>%  
  complete(Date = seq.Date(min(Date), max(Date), by="day"))

SPX_DE<-na.locf(SPX_DE, fromLast = FALSE)
SPX_DE<- SPX_DE[SPX_DE$Date %in% FilterDates,]

##################################################################################
SPX_price <- read.csv("./HistoricalPrices.csv", header = TRUE)
SPX_price <- SPX_price[,c(1,5)]# choosing only the closing price each day

SPX_price$Date<-as.Date(SPX_price$Date, format = "%m/%d/%y")# converting the date column

SPX_price<-na.locf(SPX_price, fromLast = FALSE)

SPX_return <- SPX_price %>%
  arrange(Date) %>% # Make sure the data is sorted by date in ascending order
  mutate(Daily_Return = log(Close) - log(lag(Close))) 

days_in_year <- 252
# Calculate rolling 3-year average return
SPX_return$Rolling_3Y_Avg_Return <- rollapply(SPX_return$Daily_Return, 
                                             width = 3 * days_in_year, 
                                             FUN = mean, 
                                             by.column = TRUE, 
                                             fill = NA, 
                                             align = 'right')


SPX_return$abnormal_returns<-SPX_return$Daily_Return-SPX_return$Rolling_3Y_Avg_Return

SPX_return$lagged_return<- lag(SPX_return$Daily_Return)

SPX_return<- SPX_return %>% 
  mutate(Date = as.Date(Date)) %>%  
  complete(Date = seq.Date(min(Date), max(Date), by="day"))

SPX_return<- SPX_return[SPX_return$Date %in% FilterDates,]

######################################## SURPRISES ###########################

IR_Lead <- read.csv("./DFF.csv")
IR_Lead$Date <- as.Date(IR_Lead$DATE, format = "%Y-%m-%d") # converting the date column
IR_Lead$DFF<- lead(IR_Lead$DFF)

IR_Lead<- IR_Lead %>% 
  mutate(Date = as.Date(Date)) %>%  
  complete(Date = seq.Date(min(Date), max(Date), by="day"))

IR_Lead <-na.locf(IR_Lead, fromLast = FALSE)
IR_Lead <- IR_Lead[IR_Lead$Date %in% FilterDates,][,-1]

IR_Surprises<- IR_Lead$DFF-Fed_Futures$FORWARD

######################################################################################
########################### CONSTRUCTION OF CORE DATASET FOR REGRESSIONS #############
######################################################################################

Core$return <- SPX_return$Daily_Return
Core$lagged_return <- as.numeric(SPX_return$lagged_return)
Core$abnormal_return <- as.numeric(SPX_return$abnormal_returns)
Core$IR<- IR_Lead$DFF
Core$Surprise<-IR_Surprises
Core$debt_equity<- SPX_DE$Net_Debt_Share
```

```{r Simple Models, results='asis'}
simple_tone <- lm(abnormal_return ~ Tone, data = Core)
simple_unc <- lm(abnormal_return ~ Unc, data = Core)
simple_con <- lm(abnormal_return ~ Con, data = Core)
simple_bert <- lm(abnormal_return ~ Bert, data = Core)

stargazer(simple_tone, simple_con, simple_unc, simple_bert, column.labels = c("Tone", "Unc", "Con", "Bert"), header = F)

```

\pagebreak

```{r Models including IR, echo=TRUE}
simple_tone_IR <- lm(abnormal_return~Tone + IR, data = Core)
simple_unc_IR <- lm(abnormal_return~Unc + IR, data = Core)
simple_con_IR <- lm(abnormal_return~Con + IR, data = Core)
simple_bert_IR <- lm(abnormal_return~Bert + IR, data = Core)

stargazer(simple_bert_IR, simple_bert_IR, simple_bert_IR, simple_bert_IR,
          column.labels = c("Tone", "Unc", "Con", "Bert"), header = F)

```

```{r Models including LR}
simple_tone_LR <- lm(abnormal_return ~ Tone + IR + lagged_return, data = Core)
simple_unc_LR <- lm(abnormal_return ~ Unc + IR + lagged_return, data = Core)
simple_con_LR <- lm(abnormal_return ~ Con + IR + lagged_return, data = Core)
simple_bert_LR <- lm(abnormal_return ~ Bert + IR + lagged_return, data = Core)

stargazer(simple_tone_LR, simple_con_LR, simple_unc_LR, simple_bert_LR, column.labels = c("Tone", "Unc", "Con", "Bert"), header = F)
```

```{r Models including SUR}
simple_tone_SUR <- lm(abnormal_return ~ Tone + IR + lagged_return + Surprise, data = Core)
simple_unc_SUR <- lm(abnormal_return ~ Unc + IR + lagged_return + Surprise, data = Core)
simple_con_SUR <- lm(abnormal_return ~ Con + IR + lagged_return + Surprise, data = Core)
simple_bert_SUR <- lm(abnormal_return ~ Bert + IR + lagged_return + Surprise, data = Core)

stargazer(simple_tone_SUR, simple_unc_SUR, simple_con_SUR, simple_bert_SUR, column.labels = c("Tone", "Unc", "Con", "Bert"), header = F)

```

```{r Models including DE}
simple_tone_DE <- lm(abnormal_return ~ Tone + IR + lagged_return + Surprise + debt_equity, data = Core)
simple_unc_DE <- lm(abnormal_return ~ Con + IR + lagged_return + Surprise + debt_equity, data = Core)
simple_con_DE <- lm(abnormal_return ~ Con + IR + lagged_return + Surprise + debt_equity, data = Core)
simple_bert_DE <- lm(abnormal_return ~ Bert + IR + lagged_return + Surprise + debt_equity, data = Core)

stargazer(simple_tone_DE, simple_unc_DE, simple_con_DE, simple_bert_DE, column.labels = c("Tone", "Unc", "Con", "Bert"), header = F)

```

```{r}
interactions_tone <- lm(abnormal_return ~ Tone * debt_equity + IR + lagged_return + Surprise * debt_equity + debt_equity, data = Core)

interactions_unc<-lm(abnormal_return~Unc*debt_equity + IR + lagged_return + Surprise*debt_equity + debt_equity, data=Core)

interactions_con<-lm(abnormal_return~Con*debt_equity + IR + lagged_return + Surprise*debt_equity + debt_equity, data=Core)

interactions_bert <- lm(abnormal_return~Bert*debt_equity + IR + lagged_return + Surprise*debt_equity + debt_equity, data=Core)

stargazer(interactions_tone, interactions_unc, interactions_con, interactions_bert, column.labels = c("Tone", "Unc", "Con", "Bert"), header = F)

```

```{r "Trial", results='asis'}
trial_tone<-lm(abnormal_return~Tone + IR + lagged_return + Surprise + debt_equity, data=Core)
trial_unc<-lm(abnormal_return~Unc + IR + lagged_return + Surprise + debt_equity, data=Core)
trial_con<-lm(abnormal_return~Con + IR + lagged_return + Surprise + debt_equity, data=Core)
trial_bert<-lm(abnormal_return~Bert + IR + lagged_return + Surprise + debt_equity, data=Core)

stargazer(trial_tone, trial_unc, trial_con, trial_bert,
          column.labels = c("Tone", "Unc", "Con", "Bert"), header = F)
```

## BMA 

After regressing everything and getting the results we do a final check via Baysian Model Averaging, where we can see which of the Variables from our Core data set truly are important to abnormal returns. This methodology is to check which variables have variable importance in a regression.
```{r}
install.packages("BMS")
library(BMS)


# Defining the Model
abnormal_return_model <- Core$abnormal_return ~ Core$Tone + Core$Unc + Core$Con + Core$Bert + Core$IR + Core$lagged_return + Core$Surprise + Core$debt_equity

# 
bms_results <- bms(abnormal_return_model)

# Zeige die Ergebnisse
summary(bms_results)


```


## Bibliography {#sec-bibliography}
