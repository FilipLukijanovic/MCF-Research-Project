---
title: "Does FED Communication cause immediate and abnormal returns stock market?"
author:
- Matthias Farngruber, 11801413
- Filip Lukijanovic, 11776896
- Peter Prlleshi, 11776041
output:
  pdf_document:
    latex_engine: pdflatex
  html_document: default
  word_document: default
subtitle: A sentiment analysis on the S&P 500 Stock Market Index
nocite: '@*'
bibliography: references.bib
---

```{=tex}
\tableofcontents
\pagebreak
```
## Abstract {#sec-abstract}

\pagebreak

## 1. Literature Review {#sec-literature-review}

The following research project is mostly based on the work done by Möller and Reichmann (2021) in the field of sentiment analysis. In their paper "ECB Language and Stock Returns - A Textual analysis of ECB Press Conferences" they explore the impact of the language used by the ECB in their regular press conferences on stock returns in the Euro Area.

They achieve this by mining the statements and ranking the sentiments of each statement based on general tone, uncertainty and constraint. Once this is classified, the researchers then cross-check the high frequency intraday data for Euro Area stock returns on each statement day, by employing the technique of an event study. This allows them to see how stock returns reacted to statements by the ECB at whatever point they wanted to examine them.

As mentioned, the researchers focused on tone, uncertainty and constraining language for the sentiment analysis. Tone captures the overall language - or the overarching narrative - of a statement, uncertainty measures how ambiguous a statement may be and constraining language quantifies how constraining the ECB communicates to be in the future. Of course the researchers did not read through every single statement by the ECB, instead, they employed a dictionary-based sentiment mining approach that considered grammatical and syntactical cues to analyze the sentiment expressed in ECB press conferences. Afterwards they scored each statement with regards to each category by employing heuristic-adjusted sentiment scores based on word lists used in previous studies. @möller2021 The Authors had used a plethora of controls in their regression analyses, an approach that we were more reluctant to follow due to our slightly smaller sample size. The main reason behind that is that our sample could hardly accommodate the sheer number of variables used in the intitial analysis. Below, we delve into the subset of data used, and into our methodology for modelling Möller and Reichmann's (2021) sentiment methodology. In addition, we explore whether transformer-based approaches that are bespoke to Federal Reserve Statements can outperform heuristics-adjusted sentiment approaches using dictionaries, i.e. our proprietary approach. We do so by investigating the explanatory power of regressions using each sentiment approach.

## 2. Data {#sec-data}

The data used for this project consists of the following:

-   Federal Reserve (FED) Meetings: The U.S. American FED holds regular meetings multiple times per year where they talk about the current economic situation and what the plan is going forwards. This leaves us with 74 statements over 10 years from January 30th 2013 until July 26th 2023.

-   Standard & Poor's 500 stock market index pricing: We cross-check the statements and their nature with the price of the S&P 500 stock market index on each day of a statement. This index is of particular interest since it incorporates 500 U.S. american companies representing a large part of the whole market itself. This means that we can use the S&P 500 as a proxy for how the market behaves at a given day. Alongside that we obtained weighted debt to equity ratios of the index to see if higher leverage leads to stronger reactions to sentiment. A third and final element of S&P 500 data was lagged returns that were used in some of our analysis

-   Federal Funds Futures data: the rate or price equivalent of the interest rate that the market prices in for the next 30 days was obtained as well, as a proxy for market expectations on interest rates.

All of our data, save for the FOMC meeting minutes were obtained via the Bloomberg Terminal data services, whereas the FOMC minutes were manually extracted.

## 3. Methodology {#sec-methodology}

Our analysis of equity returns following FOMC sentiment levels can be partitioned in two. The first part is sentiment extraction, wit our approach closely following that of Möller and Reichmann (2021), as well as an implementation of Google's FinBERT NLP transformer for an alternative gaugue of the tone of FOMC minutes. The second part focuses on the construction of the variables needed in our regression, both core variables as are equity returns and sentiment, as well as various controls inspired by the paper from Möller and Reichmann.

### 3.1 Sentiment Extraction - VADER Method {#sec-3.1-sentiment-extraction---vader-method}

Our first approach to modelling the sentiment of FOMC minutes closely resembles that of Möller and Reichmann (2021), who use the NLTK VADER package in Python to extract sentiment. The VADER package is special insofar as it adjusts sentiment for heuristics, which gives it an advantage over conventional dictionary-based NLP methods (Hutto, 2014) @hutto2014vader. Furthermore, sentiment can be modeled for intensity as well, with Sentiment $S \in [-4,4]$. Möller and Reichmann (2021) expand the inbuilt dictionary of words and corresponding stand-alone sentiment scores by financial terms that correspond to mere Tone, Uncertainty, as well as Constraining language, so as to be able to capture different dimensions of speech.

Our approach to this was to update the lexicon in VADER as well, whereby our extraction of Tone merely necessitated the updating of the pre-existing lexicon. For Unc and Con, however, we used the libraries by Bodnaruk (2015) @bodnaruk2015using for constraining language and Loughran and McDonald (2011) @loughran2011liability to construct scores for uncertain language and for extending the VADER dictionary by financial terms. To extract the nuances of uncertain and constraining language, the sentiment scores of all tokens not included in the respective external libraries received reduced sentiment scores, to 20% of their initial sentiment score. Meanwhile, the tokens from the libraries were weighted regularly, i.e. with sentiment scores $S \in \{ 2, -2 \}$, as 2 is equivalent to moderate intensity in the VADER package. Adjusting every word's sentiment score manually for its intensity was not deemed feasible for the scope of a project.

Once these transformations took place, the FOMC data was tokenized into sentences, which were then tokenized for each constituent word. The sentiment score for every sentence is weighted by the word count in each sentence relative to the total word count in the statement.

Finally, we constructed the three sentiment elements and saw their trajectory move as follows:
```{=tex}
\begin{figure}[ht]
\centering
\includegraphics[width=1\textwidth]{Sentiment_andBert_Fed.png}
\caption{Sentiment Trajectory}
\end{figure}
```
### 3.2 Sentiment Extraction - FinBERT Method {#sec-3.2-sentiment-extraction---finbert-method}

Another route we took to sentiment construction was that of FinBERT. FinBERT is a transformer-based natural language processing model that can analyze phrases contextually by looking at both preceding and succeeding words, i.e. it is bidirectional, which our VADER approach is not. FinBERT is an adaptation of BERT which was a generalized sentiment analysis tool developed by Google, whereby Araci (2019) @araci2019finbert trained it on an extensive corpus of financial data. Luckily for us, Chen et al. (2023) @chen2023finbert took it one step further and trained BERT on FOMC statement sentiment, which seemed like an ideal approach to pit our tone construction against in our analysis. Similarly to the approach with VADER, we constructed sentiments by BERT on a sentence by sentence basis and weighted every sentence by its relative word count.

As a next step, we had to transform and prepare all of our financial data for analysis, which we delve into below.

### 3.3 Financial Data Processing {#sec-3.3-financial-data-processing}

For every data point of our financial data, the last available datapoint was carried forward to the day of the FOMC minute issue, whereby returns were constructed via log-returns, in order to facilitate the computation of average returns in order to calculate abnormal returns. The abnormal returns, our independent variable of interest, was constructed using the model of MacKinlay (1997) @mackinlay1997event, who computed abnormal returns as the of cumulative returns after an event over average returns over a period, in our case 3 years.

Furthermore, we constructed a variable which captures the surprise effect of interest rate changes, in line with Möller and Reichmann's (2021) method in order to truly isolate the effect of sentiment from the mechanical effect of interest rate levels. This variable was constructed via the difference of implied interest rate expectations from federal funds rate futures one day before the announcement and the actual interest rate that was announced.

The point where we depart from the methodology of Möller and Reichmann (2021) is the fact that we also investigate the effects on returns for heterogeneity based on general debt levels of companies, essentially posing the question if returns react to negative sentiment more in times of high indebtedness.

### 3.4 Regression Sepcifications {#sec-3.4-regression-sepcifications}

```{r library setup, include=FALSE}
library(readxl)
library(tidyr)
library(zoo)
library(dplyr)
library(stargazer)
library(lmtest)
library(BMS)
library(writexl)
```

```{r Definition of Data, include=FALSE}

setwd(dirname(rstudioapi::getSourceEditorContext()$path))
path<-dirname(rstudioapi::getSourceEditorContext()$path)

Core <- read_excel("./SENTIMENTS_FOMC_FINAL_with_BERT.xlsx")
Core$Date <- as.Date(Core$Date, format = "%d/%m/%Y") # converting the date column
FilterDates<-c(Core$Date)
Core <- Core[, -2]

#################################################################################
Fed_Futures <- read_excel("./FED_FORWARDS.xlsx")
Fed_Futures$Date <- as.Date(Fed_Futures$Date, format="%Y-%m-%d %H:%M:%S")

Fed_Futures<- Fed_Futures %>% 
  mutate(Date = as.Date(Date)) %>%  
  complete(Date = seq.Date(min(Date), max(Date), by="day"))
      
Fed_Futures<-na.locf(Fed_Futures, fromLast = FALSE)
Fed_Futures<- Fed_Futures[Fed_Futures$Date %in% FilterDates,]

#####################################################################################

IR <- read.csv("./DFF.csv")
IR$Date <- as.Date(IR$DATE, format = "%Y-%m-%d") # converting the date column

IR<- IR %>% 
  mutate(Date = as.Date(Date)) %>%  
  complete(Date = seq.Date(min(Date), max(Date), by="day"))

IR <-na.locf(IR, fromLast = FALSE)
IR <- IR[IR$Date %in% FilterDates,][,-1]

#########################################################################

SPX_DE <- read_excel("./SPX_DE.xlsx")

SPX_DE$Date <- as.Date(SPX_DE$Date, format="%Y-%m-%d %H:%M:%S")

SPX_DE<- SPX_DE %>% 
  mutate(Date = as.Date(Date)) %>%  
  complete(Date = seq.Date(min(Date), max(Date), by="day"))

SPX_DE<-na.locf(SPX_DE, fromLast = FALSE)
SPX_DE<- SPX_DE[SPX_DE$Date %in% FilterDates,]

##################################################################################
SPX_price <- read.csv("./HistoricalPrices.csv", header = TRUE)
SPX_price <- SPX_price[,c(1,5)]# choosing only the closing price each day

SPX_price$Date<-as.Date(SPX_price$Date, format = "%m/%d/%y")# converting the date column

SPX_price<-na.locf(SPX_price, fromLast = FALSE)

SPX_return <- SPX_price %>%
  arrange(Date) %>% # Make sure the data is sorted by date in ascending order
  mutate(Daily_Return = log(Close) - log(lag(Close))) 

days_in_year <- 252
# Calculate rolling 3-year average return
SPX_return$Rolling_3Y_Avg_Return <- rollapply(SPX_return$Daily_Return, 
                                             width = 3 * days_in_year, 
                                             FUN = mean, 
                                             by.column = TRUE, 
                                             fill = NA, 
                                             align = 'right')


SPX_return$abnormal_returns<-SPX_return$Daily_Return-SPX_return$Rolling_3Y_Avg_Return

SPX_return$lagged_return<- lag(SPX_return$Daily_Return)

SPX_return<- SPX_return %>% 
  mutate(Date = as.Date(Date)) %>%  
  complete(Date = seq.Date(min(Date), max(Date), by="day"))

SPX_return<- SPX_return[SPX_return$Date %in% FilterDates,]

######################################## SURPRISES ###########################

IR_Lead <- read.csv("./DFF.csv")
IR_Lead$Date <- as.Date(IR_Lead$DATE, format = "%Y-%m-%d") # converting the date column
IR_Lead$DFF<- lead(IR_Lead$DFF)

IR_Lead<- IR_Lead %>% 
  mutate(Date = as.Date(Date)) %>%  
  complete(Date = seq.Date(min(Date), max(Date), by="day"))

IR_Lead <-na.locf(IR_Lead, fromLast = FALSE)
IR_Lead <- IR_Lead[IR_Lead$Date %in% FilterDates,][,-1]

IR_Surprises<- IR_Lead$DFF-Fed_Futures$FORWARD

######################################################################################
########################### CONSTRUCTION OF CORE DATASET FOR REGRESSIONS #############
######################################################################################

Core$return <- SPX_return$Daily_Return
Core$lagged_return <- as.numeric(SPX_return$lagged_return)
Core$abnormal_return <- as.numeric(SPX_return$abnormal_returns)
Core$IR<- IR_Lead$DFF
Core$Surprise<-IR_Surprises
Core$debt_equity<- SPX_DE$Net_Debt_Share
```

```{r Simple Models, results='asis'}
simple_tone <- lm(abnormal_return ~ Tone, data = Core)
simple_unc <- lm(abnormal_return ~ Unc, data = Core)
simple_con <- lm(abnormal_return ~ Con, data = Core)
simple_bert <- lm(abnormal_return ~ Bert, data = Core)

stargazer(simple_tone, simple_con, simple_unc, simple_bert, column.labels = c("Tone", "Unc", "Con", "Bert"), header = F)

```

\pagebreak

```{r Models including IR, echo=TRUE, results='asis'}
simple_tone_IR <- lm(abnormal_return~Tone + IR, data = Core)
simple_unc_IR <- lm(abnormal_return~Unc + IR, data = Core)
simple_con_IR <- lm(abnormal_return~Con + IR, data = Core)
simple_bert_IR <- lm(abnormal_return~Bert + IR, data = Core)

stargazer(simple_bert_IR, simple_bert_IR, simple_bert_IR, simple_bert_IR,
          column.labels = c("Tone", "Unc", "Con", "Bert"), header = F)

```

```{r Models including LR, results='asis'}
simple_tone_LR <- lm(abnormal_return ~ Tone + IR + lagged_return, data = Core)
simple_unc_LR <- lm(abnormal_return ~ Unc + IR + lagged_return, data = Core)
simple_con_LR <- lm(abnormal_return ~ Con + IR + lagged_return, data = Core)
simple_bert_LR <- lm(abnormal_return ~ Bert + IR + lagged_return, data = Core)

stargazer(simple_tone_LR, simple_con_LR, simple_unc_LR, simple_bert_LR, column.labels = c("Tone", "Unc", "Con", "Bert"), header = F)
```

```{r Models including SUR, results='asis'}
simple_tone_SUR <- lm(abnormal_return ~ Tone + IR + lagged_return + Surprise, data = Core)
simple_unc_SUR <- lm(abnormal_return ~ Unc + IR + lagged_return + Surprise, data = Core)
simple_con_SUR <- lm(abnormal_return ~ Con + IR + lagged_return + Surprise, data = Core)
simple_bert_SUR <- lm(abnormal_return ~ Bert + IR + lagged_return + Surprise, data = Core)

stargazer(simple_tone_SUR, simple_unc_SUR, simple_con_SUR, simple_bert_SUR, column.labels = c("Tone", "Unc", "Con", "Bert"), header = F)

```

```{r Models including DE, results='asis'}
simple_tone_DE <- lm(abnormal_return ~ Tone + IR + lagged_return + Surprise + debt_equity, data = Core)
simple_unc_DE <- lm(abnormal_return ~ Con + IR + lagged_return + Surprise + debt_equity, data = Core)
simple_con_DE <- lm(abnormal_return ~ Con + IR + lagged_return + Surprise + debt_equity, data = Core)
simple_bert_DE <- lm(abnormal_return ~ Bert + IR + lagged_return + Surprise + debt_equity, data = Core)

stargazer(simple_tone_DE, simple_unc_DE, simple_con_DE, simple_bert_DE, column.labels = c("Tone", "Unc", "Con", "Bert"), header = F)

```

```{r, results='asis'}
interactions_tone <- lm(abnormal_return ~ Tone * debt_equity + IR + lagged_return + Surprise * debt_equity + debt_equity, data = Core)

interactions_unc<-lm(abnormal_return~Unc*debt_equity + IR + lagged_return + Surprise*debt_equity + debt_equity, data=Core)

interactions_con<-lm(abnormal_return~Con*debt_equity + IR + lagged_return + Surprise*debt_equity + debt_equity, data=Core)

interactions_bert <- lm(abnormal_return~Bert*debt_equity + IR + lagged_return + Surprise*debt_equity + debt_equity, data=Core)

stargazer(interactions_tone, interactions_unc, interactions_con, interactions_bert, column.labels = c("Tone", "Unc", "Con", "Bert"), header = F)

```

```{r "Trial", results='asis'}
trial_tone<-lm(abnormal_return~Tone + IR + lagged_return + Surprise + debt_equity, data=Core)
trial_unc<-lm(abnormal_return~Unc + IR + lagged_return + Surprise + debt_equity, data=Core)
trial_con<-lm(abnormal_return~Con + IR + lagged_return + Surprise + debt_equity, data=Core)
trial_bert<-lm(abnormal_return~Bert + IR + lagged_return + Surprise + debt_equity, data=Core)

stargazer(trial_tone, trial_unc, trial_con, trial_bert,
          column.labels = c("Tone", "Unc", "Con", "Bert"), header = F)
```

## 4. Bayesian Model Averaging {#sec-4.-bayesian-model-averaging}

After regressing everything and getting the results we do a final check via Baysian Model Averaging (BMA), where we can see which of the Variables from our Core data set truly are important to abnormal returns. This methodology is to check which variables have variable importance in a regression.

```{r}



# Defining the Model
abnormal_return_model <- Core$abnormal_return ~ Core$Tone + Core$Unc + Core$Con + Core$Bert + Core$IR + Core$lagged_return + Core$Surprise + Core$debt_equity

# Doing the BMA itself
bms_results <- bms(abnormal_return_model)

```

## 5. Potential for Future Research {#sec-5.-potential-for-future-research}

Considering the fact that FED statements have been forced to consider significant humanitarian crises such as the outbreak of the COVID-19 pandemic (First Lockdown in the U.S. began on March 12th, 2020), the invasion of Russia into Ukraine (February 24th, 2022 - immediatly after economical from COVID began the lessen) and the Hamas terrorist attack on Israel and the war following after (since October 7th, 2023), there exists an avenue for future research.

While the U.S. interest rate policy must inevitably take into account global geopolitical events, it can only incorporate them to a limited extent.This means that a discrepancy between the U.S.-focused and more "fact-based" interest rate and the more "human" statements will likely occur, as shown in the graph in Chapter 3.3. Future research could seek to find a way to quantify the difference between economically relevant word-combinations by refining linguistic analysis methods, adjusting dictionaries, and filtering out word combinations that may focus on factors irrelevant to immediate, upcoming policy, thereby emphasizing a more financially centered context.

This research potential especially becomes apparent when observing the disparity between the overall trend the SPX shows since the "COVID-Crash" in March 2020 - particularly the clear positive trend since the beginning of 2023 - and the language employed by the FED in its statements since the beginning of the recorded statements in the data used. As mentioned, future research may seek to refine the dictionaries used and isolate sentences or word-combinations related to unforeseen geopolitical events, that cannot be ignored in a statement by a governing body.

## 6. Conclusion

\pagebreak

## 7. Bibliography {#sec-bibliography}
